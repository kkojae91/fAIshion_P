{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "크롤링이미지만추출후저장.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ07OojOoXsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import glob\n",
        "# np행렬 생략 부분 출력 펼쳐서 보기 옵션\n",
        "np.set_printoptions(threshold=np.inf, linewidth=np.inf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z2U2z1voskr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ac1cacc-f7a4-4de4-8b44-3036ad377ebe"
      },
      "source": [
        "# GPU메모리 증가 허용하기 코드..내가 알아야할 필요가 있을지..? 뭔 이야기인지는 모르겠음.\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "class fashion_tools(object):\n",
        "    def __init__(self,imageid,model,version=1.1):\n",
        "        self.imageid = imageid\n",
        "        self.model   = model\n",
        "        self.version = version\n",
        "        \n",
        "    def get_dress(self,stack=False):\n",
        "        \"\"\"limited to top wear and full body dresses (wild and studio working)\"\"\"\n",
        "        \"\"\"takes input rgb----> return PNG\"\"\"\n",
        "        name =  self.imageid\n",
        "        file = name\n",
        "        # file = cv2.imread(name)\n",
        "        file = tf.image.resize_with_pad(file,target_height=512,target_width=512)\n",
        "        rgb  = file.numpy()\n",
        "        file = np.expand_dims(file,axis=0)/ 255.\n",
        "        seq = self.model.predict(file)\n",
        "        seq = seq[3][0,:,:,0]\n",
        "        seq = np.expand_dims(seq,axis=-1)\n",
        "        c1x = rgb*seq\n",
        "        c2x = rgb*(1-seq)\n",
        "        cfx = c1x+c2x\n",
        "        dummy = np.ones((rgb.shape[0],rgb.shape[1],1))\n",
        "        rgbx = np.concatenate((rgb,dummy*255),axis=-1)\n",
        "        rgbs = np.concatenate((cfx,seq*255.),axis=-1)\n",
        "        if stack:\n",
        "            stacked = np.hstack((rgbx,rgbs))\n",
        "            return stacked\n",
        "        else:\n",
        "            return rgbs\n",
        "        \n",
        "    def get_patch(self):\n",
        "        return None\n",
        "\n",
        "    def development(self):\n",
        "        n_ = '\\n'\n",
        "        return (f\"VERSION : {self.version} {n_} Interesting tools to be added to the workflow pipe!!!\")\n",
        "\n",
        "# 이미지 패딩값 주는 코드\n",
        "def resize_with_padding(img, expected_size):\n",
        "    img.thumbnail((expected_size[0], expected_size[1]))\n",
        "    # print(img.size)\n",
        "    delta_width = expected_size[0] - img.size[0]\n",
        "    delta_height = expected_size[1] - img.size[1]\n",
        "    pad_width = delta_width // 2\n",
        "    pad_height = delta_height // 2\n",
        "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
        "    return ImageOps.expand(img, padding)\n",
        "\n",
        "# 결과 값 이미지 자르기\n",
        "def crop(image):\n",
        "    image_crop = image[:,512:]\n",
        "    image_crop = np.uint8(image_crop)\n",
        "    return black(image_crop)\n",
        "\n",
        "# 투명값 검정배경 입혀주기.\n",
        "def black(image_crop):\n",
        "    for i in image_crop:\n",
        "        for j in i:\n",
        "            if j[3] < 80:\n",
        "                j[0] = 0\n",
        "                j[1] = 0\n",
        "                j[2] = 0\n",
        "    image_black = image_crop\n",
        "    # cv2_imshow(image_black)\n",
        "    return resize(image_black)\n",
        "\n",
        "# 리사이즈 인풋값 맞춰 주기.\n",
        "def resize(image_black):\n",
        "    image_resize = cv2.resize(image_black, (28,28))\n",
        "    return gray(image_resize)\n",
        "\n",
        "# 그레이 스케일.. 후 픽셀 출력..\n",
        "def gray(image_resize):\n",
        "    image_gray = cv2.cvtColor(image_resize, cv2.COLOR_BGRA2GRAY)\n",
        "    return image_gray\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sample_images_path = glob.glob(\"/content/drive/My Drive/Colab Notebooks/blazer_images_choice/*.jpg\")\n",
        "    # 훈련된 모델 가지고 오는 경로 설정.\n",
        "    saved = load_model(\"/content/drive/My Drive/Colab Notebooks/데이터/topwears.h5\")\n",
        "\n",
        "    for idx, image_path in enumerate(sample_images_path):\n",
        "        img = Image.open(image_path)\n",
        "        img = resize_with_padding(img, (300, 300))\n",
        "        img = np.array(img)\n",
        "    \n",
        "        ###running code\n",
        "        api    = fashion_tools(img,saved)\n",
        "        image = api.get_dress(True)\n",
        "        result_image = crop(image)\n",
        "\n",
        "        # 파일저장.\n",
        "        cv2.imwrite(f'/content/drive/My Drive/Colab Notebooks/preprocessing_images/preprocessing_blazer_images/prepro_blazer_image{idx}.jpg', result_image)\n",
        "        if idx % 50 == 0:\n",
        "            print(idx,'done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "0 done\n",
            "10 done\n",
            "20 done\n",
            "30 done\n",
            "40 done\n",
            "50 done\n",
            "60 done\n",
            "70 done\n",
            "80 done\n",
            "90 done\n",
            "100 done\n",
            "110 done\n",
            "120 done\n",
            "130 done\n",
            "140 done\n",
            "150 done\n",
            "160 done\n",
            "170 done\n",
            "180 done\n",
            "190 done\n",
            "200 done\n",
            "210 done\n",
            "220 done\n",
            "230 done\n",
            "240 done\n",
            "250 done\n",
            "260 done\n",
            "270 done\n",
            "280 done\n",
            "290 done\n",
            "300 done\n",
            "310 done\n",
            "320 done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2b270f7b4267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m###running code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mapi\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mfashion_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mresult_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-2b270f7b4267>\u001b[0m in \u001b[0;36mget_dress\u001b[0;34m(self, stack)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# file = cv2.imread(name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_with_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mrgb\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mresize_image_with_pad_v2\u001b[0;34m(image, target_height, target_width, method, antialias)\u001b[0m\n\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m   return _resize_image_with_pad_common(image, target_height, target_width,\n\u001b[0;32m-> 1705\u001b[0;31m                                        _resize_fn)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36m_resize_image_with_pad_common\u001b[0;34m(image, target_height, target_width, resize_fn)\u001b[0m\n\u001b[1;32m   1564\u001b[0m       raise ValueError(\n\u001b[1;32m   1565\u001b[0m           \u001b[0;34m'\\'image\\' (shape %s) must have either 3 or 4 dimensions.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m           image_shape)\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0massert_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CheckAtLeast3DImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_static\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'image' (shape (300, 300)) must have either 3 or 4 dimensions."
          ]
        }
      ]
    }
  ]
}